{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "148c9a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import List, Dict, DefaultDict\n",
    "from gym.spaces import Space\n",
    "from gym.spaces.utils import flatdim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92910c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_greedy_policy(policy, rewards, values, valid_actions, gamma=0.9):\n",
    "    \"\"\"Improve policy (take greedy actions) with respect to rewards and state values.\n",
    "\n",
    "  Args:\n",
    "      policy (np.array): Policy giving the probability of taking each action from each state.\n",
    "      rewards (np.array): Rewards corresponding to reaching the next state from each state (so r(s,s') here rather than the usual r(s,a)).\n",
    "      values (np.array): State values evaluated for current policy.\n",
    "      gamma (float, optional): Discount factor. Defaults to 0.9.\n",
    "\n",
    "  Returns:\n",
    "      policy (np.array): Improved (greedy) policy.\n",
    "  \"\"\"\n",
    "\n",
    "    num_states, num_actions = policy.shape # 2, 3\n",
    "    greedy_policy = np.zeros_like(policy)\n",
    "    state_action_values = np.nan*policy\n",
    "    for action in range(num_actions):\n",
    "        for state in range(num_states):\n",
    "            if valid_actions[state, action]==1:\n",
    "                state_action_values[state][action] = 0.0\n",
    "                # With probability 0.9, next state = action with corresponding reward. With probability 0.1, next state = state with no reward.\n",
    "                next_state_probabilities = {action:0.9, state:0.1}\n",
    "                # Often the environment handles this next state computation (transition dynamics). But to use complete dynamic programming we must know \n",
    "                # the transition probablities.\n",
    "        for next_state in next_state_probabilities.keys():\n",
    "          # Note the expectation is now only over next state probabilities (environment dynamics) since we want the value for each valid state and action.\n",
    "            state_action_values[state][action] += next_state_probabilities[next_state]*(rewards[state][next_state]+gamma*values[next_state])\n",
    "            \n",
    "    \n",
    "    greedy_action = np.nanargmax(state_action_values[state]) # Argmax ignoring invalid actions with nan value\n",
    "  \n",
    "    for action in range(num_actions):\n",
    "        greedy_policy[state][action] = 1 if action == greedy_action else 0\n",
    "\n",
    "    print(f'State action values with previous policy:\\n {state_action_values}\\n')\n",
    "    print(f'Greedy policy after policy improvement:\\n {greedy_policy}')\n",
    "    return greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d4a1aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "    \"\"\"Base class for Q-Learning agent\n",
    "\n",
    "    **ONLY CHANGE THE BODY OF THE act() FUNCTION**\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space: Space,\n",
    "        obs_space: Space,\n",
    "        gamma: float,\n",
    "        epsilon: float,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Constructor of base agent for Q-Learning\n",
    "\n",
    "        Initializes basic variables of the Q-Learning agent\n",
    "        namely the epsilon, learning rate and discount rate.\n",
    "\n",
    "        :param action_space (int): action space of the environment\n",
    "        :param obs_space (int): observation space of the environment\n",
    "        :param gamma (float): discount factor (gamma)\n",
    "        :param epsilon (float): epsilon for epsilon-greedy action selection\n",
    "\n",
    "        :attr n_acts (int): number of actions\n",
    "        :attr q_table (DefaultDict): table for Q-values mapping (OBS, ACT) pairs of observations\n",
    "            and actions to respective Q-values\n",
    "        \"\"\"\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.obs_space = obs_space\n",
    "        self.n_acts = flatdim(action_space)\n",
    "\n",
    "        self.epsilon: float = epsilon\n",
    "        self.gamma: float = gamma\n",
    "\n",
    "        self.q_table: DefaultDict = defaultdict(lambda: 0)\n",
    "\n",
    "    def act(self, obs: int) -> int:\n",
    "        \"\"\"Implement the epsilon-greedy action selection here\n",
    "\n",
    "        :param obs (int): received observation representing the current environmental state\n",
    "        :return (int): index of selected action\n",
    "        \"\"\"\n",
    "        ### PUT YOUR CODE HERE ###\n",
    "        act_vals = [self.q_table[(obs, act)] for act in range(self.n_acts)]\n",
    "        max_val = max(act_vals)\n",
    "        max_acts = [idx for idx, act_val in enumerate(act_vals) if act_val == max_val]\n",
    "\n",
    "        final_return = 0\n",
    "        if random.random() < self.epsilon:\n",
    "            final_return = random.randint(0, self.n_acts - 1)\n",
    "        else:\n",
    "            final_return = random.choice(max_acts)\n",
    "            \n",
    "        return final_return\n",
    "        \n",
    "        ### RETURN AN ACTION HERE ###\n",
    "#         return -1\n",
    "\n",
    "    @abstractmethod\n",
    "    def schedule_hyperparameters(self, timestep: int, max_timestep: int):\n",
    "        \"\"\"Updates the hyperparameters\n",
    "\n",
    "        This function is called before every episode and allows you to schedule your\n",
    "        hyperparameters.\n",
    "\n",
    "        :param timestep (int): current timestep at the beginning of the episode\n",
    "        :param max_timestep (int): maximum timesteps that the training loop will run for\n",
    "        \"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def learn(self):\n",
    "        ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fbbc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54541d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e16361a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "618adb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    \"\"\"Agent using the Q-Learning algorithm\"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float, **kwargs):\n",
    "        \"\"\"Constructor of QLearningAgent\n",
    "\n",
    "        Initializes some variables of the Q-Learning agent, namely the epsilon, discount rate\n",
    "        and learning rate alpha.\n",
    "\n",
    "        :param alpha (float): learning rate alpha for Q-learning updates\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha: float = alpha\n",
    "\n",
    "    def learn(\n",
    "        self, obs: int, action: int, reward: float, n_obs: int, done: bool\n",
    "    ) -> float:\n",
    "        \"\"\"Updates the Q-table based on agent experience\n",
    "\n",
    "        **YOU MUST IMPLEMENT THIS FUNCTION FOR Q2**\n",
    "\n",
    "        :param obs (int): received observation representing the current environmental state\n",
    "        :param action (int): index of applied action\n",
    "        :param reward (float): received reward\n",
    "        :param n_obs (int): received observation representing the next environmental state\n",
    "        :param done (bool): flag indicating whether a terminal state has been reached\n",
    "        :return (float): updated Q-value for current observation-action pair\n",
    "        \"\"\"\n",
    "        ### PUT YOUR CODE HERE ###\n",
    "#         raise NotImplementedError(\"Needed for Q2\")\n",
    "        def flatten(actions) :\n",
    "            new_actions = [] \n",
    "            for action in actions :\n",
    "                if type(action) == list :\n",
    "                    new_actions += action\n",
    "                elif type(action) == int :\n",
    "                    new_actions.append(action)\n",
    "            return new_actions\n",
    "\n",
    "        def get_actions(possible_actions) :\n",
    "            if len(possible_actions) == 1 :\n",
    "                return possible_actions\n",
    "            pairs = []\n",
    "            for action in possible_actions[0]:\n",
    "                for action2 in possible_actions[1]:\n",
    "                    pairs.append(flatten([action, action2]))\n",
    "            new_possible_actions = [pairs] + possible_actions[2 : ]\n",
    "            possible_action_vectors = get_actions(new_possible_actions)\n",
    "            return possible_action_vectors\n",
    "\n",
    "        n_action = (flatdim(env.action_space))\n",
    "        possible_actions = [list(range(k, (k + 1))) for k in range(n_action)]\n",
    "        action_poss = get_actions(possible_actions)[0][0]\n",
    "\n",
    "        max_Q = [self.q_table[(n_obs, a)] for a in action_poss]\n",
    "        \n",
    "        target_value = reward + self.gamma * (1 - done) * max(max_Q)\n",
    "        self.q_table[(obs, action)] += self.alpha * (\n",
    "            target_value - self.q_table[(obs, action)]\n",
    "        )\n",
    "        \n",
    "        return self.q_table[(obs, action)]\n",
    "\n",
    "    def schedule_hyperparameters(self, timestep: int, max_timestep: int):\n",
    "        \"\"\"Updates the hyperparameters\n",
    "\n",
    "        **DO NOT CHANGE THE PROVIDED SCHEDULING WHEN TESTING PROVIDED HYPERPARAMETER PROFILES IN Q2**\n",
    "\n",
    "        This function is called before every episode and allows you to schedule your\n",
    "        hyperparameters.\n",
    "\n",
    "        :param timestep (int): current timestep at the beginning of the episode\n",
    "        :param max_timestep (int): maximum timesteps that the training loop will run for\n",
    "        \"\"\"\n",
    "        self.epsilon = 1.0 - (min(1.0, timestep / (0.20 * max_timestep))) * 0.99\n",
    "\n",
    "\n",
    "class MonteCarloAgent(Agent):\n",
    "    \"\"\"Agent using the Monte-Carlo algorithm for training\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"Constructor of MonteCarloAgent\n",
    "\n",
    "        Initializes some variables of the Monte-Carlo agent, namely epsilon,\n",
    "        discount rate and an empty observation-action pair dictionary.\n",
    "\n",
    "        :attr sa_counts (Dict[(Obs, Act), int]): dictionary to count occurrences observation-action pairs\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.sa_counts = {}\n",
    "\n",
    "    def learn(\n",
    "        self, obses: List[int], actions: List[int], rewards: List[float]\n",
    "    ) -> Dict:\n",
    "        \"\"\"Updates the Q-table based on agent experience\n",
    "\n",
    "        **YOU MUST IMPLEMENT THIS FUNCTION FOR Q2**\n",
    "\n",
    "        :param obses (List(int)): list of received observations representing environmental states\n",
    "            of trajectory (in the order they were encountered)\n",
    "        :param actions (List[int]): list of indices of applied actions in trajectory (in the\n",
    "            order they were applied)\n",
    "        :param rewards (List[float]): list of received rewards during trajectory (in the order\n",
    "            they were received)\n",
    "        :return (Dict): A dictionary containing the updated Q-value of all the updated state-action pairs\n",
    "            indexed by the state action pair.\n",
    "        \"\"\"\n",
    "        updated_values = {}\n",
    "#         print(obses, actions, rewards)\n",
    "\n",
    "        N = len(obses)\n",
    "        T = len(rewards)\n",
    "            \n",
    "        # generate all combinations\n",
    "        all_pairs = []\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                # ensure each pair is unique\n",
    "                if [obses[i],actions[j]] not in all_pairs:\n",
    "                    all_pairs.append([obses[i],actions[j]])\n",
    "\n",
    "        returns_sa = np.zeros(T)\n",
    "        \n",
    "#         # what is the return????\n",
    "#         for elem in all_pairs:\n",
    "#             returns_sa[elem] = \n",
    "#             index = all_pairs.index(elem)\n",
    "#             average = sum(rewards[0:index+1]) / len(rewards[0:index+1])\n",
    "#             updated_values[index] = average        \n",
    "            \n",
    "        # what is the return????\n",
    "        for elem in all_pairs:\n",
    "            index = all_pairs.index(elem)\n",
    "            G = 0\n",
    "            G = sum(rewards[0:index])\n",
    "            average = G / (index+1)\n",
    "            updated_values[index] = average  \n",
    "        \n",
    "        ### PUT YOUR CODE HERE ###\n",
    "#         raise NotImplementedError(\"Needed for Q2\")\n",
    "        return updated_values\n",
    "\n",
    "    def schedule_hyperparameters(self, timestep: int, max_timestep: int):\n",
    "        \"\"\"Updates the hyperparameters\n",
    "\n",
    "        **DO NOT CHANGE THE PROVIDED SCHEDULING WHEN TESTING PROVIDED HYPERPARAMETER PROFILES IN Q2**\n",
    "\n",
    "        This function is called before every episode and allows you to schedule your\n",
    "        hyperparameters.\n",
    "\n",
    "        :param timestep (int): current timestep at the beginning of the episode\n",
    "        :param max_timestep (int): maximum timesteps that the training loop will run for\n",
    "        \"\"\"\n",
    "        self.epsilon = 1.0 - (min(1.0, timestep / (0.9 * max_timestep))) * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d61156e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 1130/10000 [00:03<00:36, 244.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 1000 - MEAN RETURN -100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2071/10000 [00:05<00:38, 208.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 2000 - MEAN RETURN -99.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3068/10000 [00:08<00:27, 248.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 3000 - MEAN RETURN -90.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 4269/10000 [00:09<00:09, 590.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 4000 - MEAN RETURN -58.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 5330/10000 [00:11<00:05, 932.39it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 5000 - MEAN RETURN -26.808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 6380/10000 [00:11<00:02, 1305.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 6000 - MEAN RETURN -3.332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 7304/10000 [00:12<00:02, 1223.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 7000 - MEAN RETURN 4.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 8413/10000 [00:13<00:01, 1536.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 8000 - MEAN RETURN 6.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 9333/10000 [00:13<00:00, 1357.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 9000 - MEAN RETURN 7.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:14<00:00, 691.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 10000 - MEAN RETURN 7.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "from constants import EX2_QL_CONSTANTS as CONSTANTS\n",
    "# from agents import QLearningAgent\n",
    "from utils import evaluate\n",
    "\n",
    "CONFIG = {\n",
    "    \"eval_freq\": 1000, # keep this unchanged\n",
    "    \"alpha\": 0.05,\n",
    "    \"epsilon\": 0.9,\n",
    "    \"gamma\": 0.99,\n",
    "}\n",
    "CONFIG.update(CONSTANTS)\n",
    "\n",
    "\n",
    "def q_learning_eval(\n",
    "        env,\n",
    "        config,\n",
    "        q_table,\n",
    "        render=False,\n",
    "        output=True):\n",
    "    \"\"\"\n",
    "    Evaluate configuration of Q-learning on given environment when initialised with given Q-table\n",
    "\n",
    "    :param env (gym.Env): environment to execute evaluation on\n",
    "    :param config (Dict[str, float]): configuration dictionary containing hyperparameters\n",
    "    :param q_table (Dict[(Obs, Act), float]): Q-table mapping observation-action to Q-values\n",
    "    :param render (bool): flag whether evaluation runs should be rendered\n",
    "    :param output (bool): flag whether mean evaluation performance should be printed\n",
    "    :return (float, float): mean and standard deviation of returns received over episodes\n",
    "    \"\"\"\n",
    "    eval_agent = QLearningAgent(\n",
    "        action_space=env.action_space,\n",
    "        obs_space=env.observation_space,\n",
    "        gamma=config[\"gamma\"],\n",
    "        alpha=config[\"alpha\"],\n",
    "        epsilon=0.0,\n",
    "    )\n",
    "    eval_agent.q_table = q_table\n",
    "    return evaluate(env, eval_agent, config[\"eval_eps_max_steps\"], config[\"eval_episodes\"], render)\n",
    "\n",
    "\n",
    "def train(env, config, output=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate Q-Learning on given environment with provided hyperparameters\n",
    "\n",
    "    :param env (gym.Env): environment to execute evaluation on\n",
    "    :param config (Dict[str, float]): configuration dictionary containing hyperparameters\n",
    "    :param output (bool): flag if mean evaluation results should be printed\n",
    "    :return (float, List[float], List[float], Dict[(Obs, Act), float]):\n",
    "        total reward over all episodes, list of means and standard deviations of evaluation\n",
    "        returns, final Q-table\n",
    "    \"\"\"\n",
    "    agent = QLearningAgent(\n",
    "        action_space=env.action_space,\n",
    "        obs_space=env.observation_space,\n",
    "        gamma=config[\"gamma\"],\n",
    "        alpha=config[\"alpha\"],\n",
    "        epsilon=config[\"epsilon\"],\n",
    "    )\n",
    "\n",
    "    step_counter = 0\n",
    "    max_steps = config[\"total_eps\"] * config[\"eps_max_steps\"]\n",
    "\n",
    "    total_reward = 0\n",
    "    evaluation_return_means = []\n",
    "    evaluation_negative_returns = []\n",
    "\n",
    "    for eps_num in tqdm(range(1, config[\"total_eps\"]+1)):\n",
    "        obs = env.reset()\n",
    "        episodic_return = 0\n",
    "        t = 0\n",
    "\n",
    "        while t < config[\"eps_max_steps\"]:\n",
    "            agent.schedule_hyperparameters(step_counter, max_steps)\n",
    "            act = agent.act(obs)\n",
    "            n_obs, reward, done, _ = env.step(act)\n",
    "            agent.learn(obs, act, reward, n_obs, done)\n",
    "\n",
    "            t += 1\n",
    "            step_counter += 1\n",
    "            episodic_return += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            obs = n_obs\n",
    "\n",
    "        total_reward += episodic_return\n",
    "\n",
    "        if eps_num > 0 and eps_num % config[\"eval_freq\"] == 0:\n",
    "            mean_return, negative_returns = q_learning_eval(env, config, agent.q_table)\n",
    "            tqdm.write(f\"EVALUATION: EP {eps_num} - MEAN RETURN {mean_return}\")\n",
    "            evaluation_return_means.append(mean_return)\n",
    "            evaluation_negative_returns.append(negative_returns)\n",
    "\n",
    "    return total_reward, evaluation_return_means, evaluation_negative_returns, agent.q_table\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(CONFIG[\"env\"])\n",
    "    total_reward, _, _, q_table = train(env, CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1f057e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5076/100000 [00:14<09:26, 167.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 5000 - MEAN RETURN -384.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10105/100000 [00:28<09:11, 162.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 10000 - MEAN RETURN -391.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15100/100000 [00:41<08:23, 168.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 15000 - MEAN RETURN -389.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20068/100000 [00:55<09:07, 145.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 20000 - MEAN RETURN -392.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25066/100000 [01:09<08:32, 146.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 25000 - MEAN RETURN -387.822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30094/100000 [01:23<06:51, 170.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 30000 - MEAN RETURN -388.746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35111/100000 [01:37<06:33, 164.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 35000 - MEAN RETURN -392.102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40082/100000 [01:51<05:59, 166.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 40000 - MEAN RETURN -390.736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45100/100000 [02:05<05:47, 157.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 45000 - MEAN RETURN -389.426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50058/100000 [02:19<06:37, 125.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 50000 - MEAN RETURN -391.256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55108/100000 [02:33<04:13, 176.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 55000 - MEAN RETURN -391.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60052/100000 [02:46<04:54, 135.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 60000 - MEAN RETURN -388.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65096/100000 [03:00<03:42, 156.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 65000 - MEAN RETURN -393.226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70044/100000 [03:14<03:41, 135.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 70000 - MEAN RETURN -389.858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75079/100000 [03:28<02:15, 183.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 75000 - MEAN RETURN -393.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80147/100000 [03:42<01:42, 192.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 80000 - MEAN RETURN -393.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85075/100000 [03:56<01:23, 178.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 85000 - MEAN RETURN -388.392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90078/100000 [04:09<01:08, 145.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 90000 - MEAN RETURN -387.368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95109/100000 [04:23<00:27, 178.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 95000 - MEAN RETURN -389.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [04:37<00:00, 360.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: EP 100000 - MEAN RETURN -392.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from constants import EX2_MC_CONSTANTS as CONSTANTS\n",
    "# from agents import MonteCarloAgent\n",
    "from utils import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "CONFIG = {\n",
    "    \"eval_freq\": 5000, # keep this unchanged\n",
    "    \"epsilon\": 0.9,\n",
    "    \"gamma\": 0.99,\n",
    "}\n",
    "CONFIG.update(CONSTANTS)\n",
    "\n",
    "\n",
    "\n",
    "def monte_carlo_eval(\n",
    "        env,\n",
    "        config,\n",
    "        q_table,\n",
    "        render=False):\n",
    "    \"\"\"\n",
    "    Evaluate configuration of MC on given environment when initialised with given Q-table\n",
    "\n",
    "    :param env (gym.Env): environment to execute evaluation on\n",
    "    :param config (Dict[str, float]): configuration dictionary containing hyperparameters\n",
    "    :param q_table (Dict[(Obs, Act), float]): Q-table mapping observation-action to Q-values\n",
    "    :param render (bool): flag whether evaluation runs should be rendered\n",
    "    :return (float, float): mean and standard deviation of returns received over episodes\n",
    "    \"\"\"\n",
    "    eval_agent = MonteCarloAgent(\n",
    "        action_space=env.action_space,\n",
    "        obs_space=env.observation_space,\n",
    "        gamma=CONFIG[\"gamma\"],\n",
    "        epsilon=0.0,\n",
    "    )\n",
    "    eval_agent.q_table = q_table\n",
    "    return evaluate(env, eval_agent, config[\"eval_eps_max_steps\"], config[\"eval_episodes\"], render)\n",
    "\n",
    "\n",
    "def train(env, config):\n",
    "    \"\"\"\n",
    "    Train and evaluate MC on given environment with provided hyperparameters\n",
    "\n",
    "    :param env (gym.Env): environment to execute evaluation on\n",
    "    :param config (Dict[str, float]): configuration dictionary containing hyperparameters\n",
    "    :return (float, List[float], List[float], Dict[(Obs, Act), float]):\n",
    "        returns over all episodes, list of means and standard deviations of evaluation\n",
    "        returns, final Q-table, final state-action counts\n",
    "    \"\"\"\n",
    "    agent = MonteCarloAgent(\n",
    "        action_space=env.action_space,\n",
    "        obs_space=env.observation_space,\n",
    "        gamma=config[\"gamma\"],\n",
    "        epsilon=config[\"epsilon\"],\n",
    "    )\n",
    "\n",
    "    step_counter = 0\n",
    "    max_steps = config[\"total_eps\"] * config[\"eps_max_steps\"]\n",
    "\n",
    "    total_reward = 0\n",
    "    evaluation_return_means = []\n",
    "    evaluation_negative_returns = []\n",
    "\n",
    "    for eps_num in tqdm(range(1, config[\"total_eps\"] + 1)):\n",
    "        obs = env.reset()\n",
    "\n",
    "        t = 0\n",
    "        episodic_return = 0\n",
    "\n",
    "        obs_list, act_list, rew_list = [], [], []\n",
    "        while t < config[\"eps_max_steps\"]:\n",
    "            agent.schedule_hyperparameters(step_counter, max_steps)\n",
    "            ##################\n",
    "            act = agent.act(obs)\n",
    "\n",
    "            n_obs, reward, done, _ = env.step(act)\n",
    "\n",
    "            obs_list.append(obs)\n",
    "            rew_list.append(reward)\n",
    "            act_list.append(act)\n",
    "\n",
    "            t += 1\n",
    "            step_counter += 1\n",
    "            episodic_return += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            obs = n_obs\n",
    "\n",
    "        agent.learn(obs_list, act_list, rew_list)\n",
    "        total_reward += episodic_return\n",
    "\n",
    "        if eps_num > 0 and eps_num % config[\"eval_freq\"] == 0:\n",
    "            mean_return, negative_returns = monte_carlo_eval(env, config, agent.q_table)\n",
    "            tqdm.write(f\"EVALUATION: EP {eps_num} - MEAN RETURN {mean_return}\")\n",
    "            evaluation_return_means.append(mean_return)\n",
    "            evaluation_negative_returns.append(negative_returns)\n",
    "\n",
    "    return total_reward, evaluation_return_means, evaluation_negative_returns, agent.q_table\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(CONFIG[\"env\"])\n",
    "    total_reward, _, _, q_table = train(env, CONFIG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
