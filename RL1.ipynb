{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9bcfeb",
   "metadata": {},
   "source": [
    "Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7dd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69140367",
   "metadata": {},
   "source": [
    "# Q1 Dynamic Programming\n",
    "- To find optimal policies for Markov Decision Processes (MDPs)\n",
    "- Implement the Policy Iteration (PI) and Value Iteration (VI) algorithms (functions).\n",
    "- For each algorithm, you can find the functions that you need to implement under Tasks below. \n",
    "- Read the code documentation to understand the input and required outputs of these functions. \n",
    "- We will mark your submission only based on the correctness of the outputs of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd33929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rl2023.exercise1.mdp_solver import ValueIteration, PolicyIteration\n",
    "from rl2023.exercise1.mdp import MDP, Transition, State, Action\n",
    "from rl2023 import constants\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "constants.EX1_CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60facd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_state_values(policy, rewards, gamma=0.9, theta=0.01):\n",
    "    \"\"\"\n",
    "  Args:\n",
    "      policy (np.array): Policy giving the probability of taking each action from each state.\n",
    "      rewards (np.array): Rewards corresponding to reaching the next state from each state (so r(s,s') here rather than the usual r(s,a)).\n",
    "      gamma (float, optional): Discount factor. Defaults to 0.9.\n",
    "      theta (float, optional): Minimum value change threshold to terminate iteration. Defaults to 0.01.\n",
    "\n",
    "  Returns:\n",
    "      values (np.array): State values for given policy and reward mappings.\n",
    "  \"\"\"    \n",
    "    num_states, num_actions = policy.shape # 2, 3\n",
    "    values = np.zeros((num_states+1)) # Include terminal state\n",
    "    \n",
    "    print(f'Beginning policy evaluation for given policy and MDP...\\n')\n",
    "    iteration = 1\n",
    "\n",
    "    while True:\n",
    "        print(f'Iteration: {iteration} \\t Current Values: {values}')\n",
    "        delta=0\n",
    "        initial_values = values\n",
    "        values = np.zeros_like(values)\n",
    "        for state in range(num_states):\n",
    "            for action in range(num_actions):\n",
    "                # With probability 0.9, next state = action with corresponding reward. With probability 0.1, next state = state with no reward.\n",
    "                next_state_probabilities = {action:0.9, state:0.1}\n",
    "                # Often we leave this next state computation (transition dynamics) to the environment. But to use complete dynamic programming we must know \n",
    "                # the transition probablities.\n",
    "                for next_state in next_state_probabilities.keys():\n",
    "                  # Note the expectation here is over both the policy and next state probabilities (environment dynamics)\n",
    "                    aa = policy[state][action]*next_state_probabilities[next_state] * (rewards[state][next_state] + gamma * initial_values[next_state])\n",
    "                    values[state] += aa\n",
    "#                     print(aa)\n",
    "\n",
    "            delta = max(delta, abs(initial_values[state]-values[state]))\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f'\\nMax difference in state value from previous iteration = {delta} which is less than threshold {theta}. Policy Evaluation terminating...\\n')\n",
    "            break\n",
    "\n",
    "        iteration+=1\n",
    "\n",
    "    print(f'Final policy state values: {values}')\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89835e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_greedy_policy(policy, rewards, values, valid_actions, gamma=0.9):\n",
    "    \"\"\"Improve policy (take greedy actions) with respect to rewards and state values.\n",
    "\n",
    "  Args:\n",
    "      policy (np.array): Policy giving the probability of taking each action from each state.\n",
    "      rewards (np.array): Rewards corresponding to reaching the next state from each state (so r(s,s') here rather than the usual r(s,a)).\n",
    "      values (np.array): State values evaluated for current policy.\n",
    "      gamma (float, optional): Discount factor. Defaults to 0.9.\n",
    "\n",
    "  Returns:\n",
    "      policy (np.array): Improved (greedy) policy.\n",
    "  \"\"\"\n",
    "\n",
    "    num_states, num_actions = policy.shape # 2, 3\n",
    "    greedy_policy = np.zeros_like(policy)\n",
    "    state_action_values = np.nan*policy\n",
    "    for action in range(num_actions):\n",
    "        for state in range(num_states):\n",
    "            if valid_actions[state, action]==1:\n",
    "                state_action_values[state][action] = 0.0\n",
    "                # With probability 0.9, next state = action with corresponding reward. With probability 0.1, next state = state with no reward.\n",
    "                next_state_probabilities = {action:0.9, state:0.1}\n",
    "                # Often the environment handles this next state computation (transition dynamics). But to use complete dynamic programming we must know \n",
    "                # the transition probablities.\n",
    "        for next_state in next_state_probabilities.keys():\n",
    "          # Note the expectation is now only over next state probabilities (environment dynamics) since we want the value for each valid state and action.\n",
    "            state_action_values[state][action] += next_state_probabilities[next_state]*(rewards[state][next_state]+gamma*values[next_state])\n",
    "            \n",
    "    \n",
    "    greedy_action = np.nanargmax(state_action_values[state]) # Argmax ignoring invalid actions with nan value\n",
    "  \n",
    "    for action in range(num_actions):\n",
    "        greedy_policy[state][action] = 1 if action == greedy_action else 0\n",
    "\n",
    "    print(f'State action values with previous policy:\\n {state_action_values}\\n')\n",
    "    print(f'Greedy policy after policy improvement:\\n {greedy_policy}')\n",
    "    return greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state values\n",
    "valid_actions = np.array([[0,1,0],[1,0,1]])\n",
    "\n",
    "policy = np.array([[0,1,0],[0.5,0,0.5]])\n",
    "\n",
    "rewards = np.array([[0,0,0],[0,0,10]])\n",
    "###############################################################\n",
    "\n",
    "# algorithm\n",
    "values = calculate_state_values(policy, rewards, gamma=0.9, theta=0.01)\n",
    "\n",
    "policy = calculate_greedy_policy(policy, rewards, values, valid_actions, gamma=0.9)\n",
    "\n",
    "# values = calculate_state_values(policy, rewards, gamma=0.9, theta=0.01)\n",
    "\n",
    "# policy = calculate_greedy_policy(policy, rewards, values, valid_actions, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af45c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration(MDPSolver):\n",
    "    \"\"\"MDP solver using the Value Iteration algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def _calc_value_func(self, theta: float) -> np.ndarray:\n",
    "        \"\"\"Calculates the value function\n",
    "\n",
    "        **YOU MUST IMPLEMENT THIS FUNCTION FOR Q1**\n",
    "\n",
    "        **DO NOT ALTER THE MDP HERE**\n",
    "\n",
    "        Useful Variables:\n",
    "        1. `self.mpd` -- Gives access to the MDP.\n",
    "        2. `self.mdp.R` -- 3D NumPy array with the rewards for each transition.\n",
    "            E.g. the reward of transition [3] -2-> [4] (going from state 3 to state 4 with action\n",
    "            2) can be accessed with `self.R[3, 2, 4]`\n",
    "        3. `self.mdp.P` -- 3D NumPy array with transition probabilities.\n",
    "            *REMEMBER*: the sum of (STATE, ACTION, :) should be 1.0 (all actions lead somewhere)\n",
    "            E.g. the transition probability of transition [3] -2-> [4] (going from state 3 to\n",
    "            state 4 with action 2) can be accessed with `self.P[3, 2, 4]`\n",
    "\n",
    "        :param theta (float): theta is the stop threshold for value iteration\n",
    "        :return (np.ndarray of float with dim (num of states)):\n",
    "            1D NumPy array with the values of each state.\n",
    "            E.g. V[3] returns the computed value for state 3\n",
    "        \"\"\"\n",
    "        V = np.zeros(self.state_dim)\n",
    "        ### PUT YOUR CODE HERE ###\n",
    "        # for each state, find the a which is maximum\n",
    "        num_states, num_actions = self.state_dim, self.action_dim \n",
    "        rewards = self.mdp.R # access rewards \n",
    "        gamma = self.gamma\n",
    "        delta = 0\n",
    "        policies = np.zeros(num_states)\n",
    "        while delta < theta:\n",
    "            for s in range(num_states):\n",
    "                v = V[s].copy()\n",
    "                # initialise an array for each iteration (state), \n",
    "                # which consists of the possible values for each action \n",
    "                find_max = np.zeros(num_actions) \n",
    "                for a in range(num_actions): \n",
    "                    for ns in range(num_states):\n",
    "                        next_state_probabilities = self.mdp.P[s, a, ns]\n",
    "                        find_max[a] += next_state_probabilities*(rewards[s, a, ns] + gamma * V[ns])\n",
    "                V[s] = max(find_max)\n",
    "                policies[s] = np.nanargmax(find_max)\n",
    "                \n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        \n",
    "#         raise NotImplementedError(\"Needed for Q1\")\n",
    "        return V\n",
    "\n",
    "    def _calc_policy(self, V: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculates the policy\n",
    "\n",
    "        **YOU MUST IMPLEMENT THIS FUNCTION FOR Q1**\n",
    "\n",
    "        :param V (np.ndarray of float with dim (num of states)):\n",
    "            A 1D NumPy array that encodes the computed value function (from _calc_value_func(...))\n",
    "            It is indexed as (State) where V[State] is the value of state 'State'\n",
    "        :return (np.ndarray of float with dim (num of states, num of actions):\n",
    "            A 2D NumPy array that encodes the calculated policy.\n",
    "            It is indexed as (STATE, ACTION) where policy[STATE, ACTION] has the probability of\n",
    "            taking action 'ACTION' in state 'STATE'.\n",
    "            REMEMBER: the sum of policy[STATE, :] should always be 1.0\n",
    "            For deterministic policies the following holds for each state S:\n",
    "            policy[S, BEST_ACTION] = 1.0\n",
    "            policy[S, OTHER_ACTIONS] = 0\n",
    "        \"\"\"\n",
    "        policy = np.zeros([self.state_dim, self.action_dim])\n",
    "        ### PUT YOUR CODE HERE ###\n",
    "        num_states, num_actions = self.state_dim, self.action_dim \n",
    "        rewards = self.mdp.R # access rewards \n",
    "        gamma = self.gamma\n",
    "# #         raise NotImplementedError(\"Needed for Q1\")\n",
    "        for s in range(num_states):\n",
    "            find_max = np.zeros(num_actions) \n",
    "            for a in range(num_actions): \n",
    "                for ns in range(num_states):\n",
    "                    next_state_probabilities = self.mdp.P[s, a, ns]\n",
    "                    find_max[a] += next_state_probabilities*(rewards[s, a, ns] + gamma * V[ns])\n",
    "            for i in range(len(find_max)):\n",
    "                if find_max[i] == V[s]:\n",
    "                    policy[s] = i\n",
    "            \n",
    "        return policy\n",
    "\n",
    "    def solve(self, theta: float = 1e-6) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Solves the MDP\n",
    "\n",
    "        Compiles the MDP and then calls the calc_value_func and\n",
    "        calc_policy functions to return the best policy and the\n",
    "        computed value function\n",
    "\n",
    "        **DO NOT CHANGE THIS FUNCTION**\n",
    "\n",
    "        :param theta (float, optional): stop threshold, defaults to 1e-6\n",
    "        :return (Tuple[np.ndarray of float with dim (num of states, num of actions),\n",
    "                       np.ndarray of float with dim (num of states)):\n",
    "            Tuple of calculated policy and value function\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mdp.ensure_compiled()\n",
    "        V = self._calc_value_func(theta)\n",
    "        policy = self._calc_policy(V)\n",
    "\n",
    "        return policy, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration(MDPSolver):\n",
    "    \"\"\"MDP solver using the Policy Iteration algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def _policy_eval(self, policy: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes one policy evaluation step\n",
    "\n",
    "        **YOU MUST IMPLEMENT THIS FUNCTION FOR Q1**\n",
    "\n",
    "        :param policy (np.ndarray of float with dim (num of states, num of actions)):\n",
    "            A 2D NumPy array that encodes the policy.\n",
    "            It is indexed as (STATE, ACTION) where policy[STATE, ACTION] has the probability of\n",
    "            taking action 'ACTION' in state 'STATE'.\n",
    "            REMEMBER: the sum of policy[STATE, :] should always be 1.0\n",
    "            For deterministic policies the following holds for each state S:\n",
    "            policy[S, BEST_ACTION] = 1.0\n",
    "            policy[S, OTHER_ACTIONS] = 0\n",
    "        :return (np.ndarray of float with dim (num of states)): \n",
    "            A 1D NumPy array that encodes the computed value function\n",
    "            It is indexed as (State) where V[State] is the value of state 'State'\n",
    "        \"\"\"\n",
    "        ### PUT YOUR CODE HERE ###\n",
    "        # initilise V(s)\n",
    "        theta = 0.01 # self.theta\n",
    "#         policy = np.zeros([self.state_dim, self.action_dim])\n",
    "#         policy = np.array([[1,0,0],[0,1,0],[0.5,0,0.5]])\n",
    "        num_states, num_actions = policy.shape # 2, 3\n",
    "    \n",
    "        V = np.zeros(self.state_dim)\n",
    "#         values = np.zeros((num_states + 1)) # Include terminal state\n",
    "        gamma = 0.9 #self.gamma # obtain gamma value\n",
    "        \n",
    "        rewards = self.mdp.R # access rewards \n",
    "#         print(rewards)\n",
    "#         print(self.mdp.P) \n",
    "\n",
    "        \n",
    "        print(f'Beginning policy evaluation for given policy and MDP...\\n')\n",
    "        iteration = 1\n",
    "\n",
    "        while True:\n",
    "            print(f'Iteration: {iteration} \\t Current Values: {V}')\n",
    "            delta = 0\n",
    "            # set v = V\n",
    "            initial_values = V.copy()\n",
    "            # values = V(s)\n",
    "#             V = np.zeros_like(V)\n",
    "            for s in range(num_states):\n",
    "                for a in range(num_actions):\n",
    "                    for ns in range(num_states):\n",
    "                        next_state_probabilities = self.mdp.P[s, a, ns] # *** how to get te next state policy?\n",
    "                      # Note the expectation here is over both the policy and next state probabilities (environment dynamics)\n",
    "                        V[s] += policy[s][a]*next_state_probabilities*(rewards[s, a, ns] + gamma * initial_values[ns])\n",
    "\n",
    "                delta = max(delta, abs(initial_values[s]-V[s]))\n",
    "        \n",
    "            if delta < theta:\n",
    "                print(f'\\nMax difference in state value from previous iteration = {delta} which is less than threshold {theta}. Policy Evaluation terminating...\\n')\n",
    "                break\n",
    "\n",
    "            iteration+=1\n",
    "\n",
    "            print(f'Final policy state values: {V}')\n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError(\"Needed for Q1\")\n",
    "        return np.array(V)\n",
    "\n",
    "    def _policy_improvement(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Computes policy iteration until a stable policy is reached\n",
    "\n",
    "        **YOU MUST IMPLEMENT THIS FUNCTION FOR Q1**\n",
    "\n",
    "        Useful Variables (As with Value Iteration):\n",
    "        1. `self.mpd` -- Gives access to the MDP.\n",
    "        2. `self.mdp.R` -- 3D NumPy array with the rewards for each transition.\n",
    "            E.g. the reward of transition [3] -2-> [4] (going from state 3 to state 4 with action\n",
    "            2) can be accessed with `self.R[3, 2, 4]`\n",
    "        3. `self.mdp.P` -- 3D NumPy array with transition probabilities.\n",
    "            *REMEMBER*: the sum of (STATE, ACTION, :) should be 1.0 (all actions lead somewhere)\n",
    "            E.g. the transition probability of transition [3] -2-> [4] (going from state 3 to\n",
    "            state 4 with action 2) can be accessed with `self.P[3, 2, 4]`\n",
    "\n",
    "        :return (Tuple[np.ndarray of float with dim (num of states, num of actions),\n",
    "                       np.ndarray of float with dim (num of states)):\n",
    "            Tuple of calculated policy and value function\n",
    "        \"\"\"\n",
    "        num_states, num_actions = self.state_dim, self.action_dim \n",
    "        policy = np.zeros([num_states, num_actions])\n",
    "#         policy = np.array([[1,0,0],[0,1,0],[0.5,0,0.5]])\n",
    "        policy = np.array([[1,0,0],[1,0,0],[1,0,0]])\n",
    "        V = np.zeros([num_states])\n",
    "        ### PUT YOUR CODE HERE ###\n",
    "#         raise NotImplementedError(\"Needed for Q1\")\n",
    "        gamma = 0.9 #self.gamma \n",
    "        rewards = self.mdp.R # access rewards \n",
    "        policy_stable = False\n",
    "        if (policy_stable == False):\n",
    "            V = self._policy_eval(policy)\n",
    "            print(V)\n",
    "            for s in range(num_states):\n",
    "                action = policy[s].copy()\n",
    "                policy_max = np.zeros(num_actions)\n",
    "                for a in range(num_actions): ### in action?\n",
    "                    for ns in range(num_states):\n",
    "                        next_state_probabilities = self.mdp.P[s, a, ns] \n",
    "                        policy_max[a] += next_state_probabilities*(rewards[s, a, ns] + gamma * V[ns])\n",
    "                        \n",
    "                # for each state, take the action which leads to maximum policy[s]\n",
    "#                 policy[s] = max(find_max_a, key=find_max_a.get)\n",
    "                policy[s] = np.nanargmax(policy_max)\n",
    "\n",
    "                # if action == policy[s], then policy_stable = true, and break loop \n",
    "                print(\"action =\", action, \"policy = \", policy[s])\n",
    "            \n",
    "                if (action == policy[s]).all():\n",
    "                    print(policy_stable)\n",
    "                    policy_stable = True\n",
    "                    break\n",
    "\n",
    "        return V, policy\n",
    "    # 3.\n",
    "    # set (random?) possible actions (a) from the policy\n",
    "    # find the best policy\n",
    "    # if a is not equal to the best policy, repeat the steps until they are equal\n",
    "\n",
    "    def solve(self, theta: float = 1e-6) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Solves the MDP\n",
    "\n",
    "        This function compiles the MDP and then calls the\n",
    "        policy improvement function that the student must implement\n",
    "        and returns the solution\n",
    "\n",
    "        **DO NOT CHANGE THIS FUNCTION**\n",
    "\n",
    "        :param theta (float, optional): stop threshold, \n",
    "        defaults to 1e-6\n",
    "        :return (Tuple[np.ndarray of float with dim (num of states, num of actions),\n",
    "                       np.ndarray of float with dim (num of states)]):\n",
    "            Tuple of calculated policy and value function\n",
    "        \"\"\"\n",
    "        self.mdp.ensure_compiled()\n",
    "        self.theta = theta\n",
    "        return self._policy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58670fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP()\n",
    "mdp.add_transition(\n",
    "    #         start action end prob reward\n",
    "    Transition(\"rock0\", \"jump0\", \"rock0\", 1, 0),\n",
    "    Transition(\"rock0\", \"stay\", \"rock0\", 1, 0),\n",
    "    Transition(\"rock0\", \"jump1\", \"rock0\", 0.1, 0),\n",
    "    Transition(\"rock0\", \"jump1\", \"rock1\", 0.9, 0),\n",
    "    Transition(\"rock1\", \"jump0\", \"rock1\", 0.1, 0),\n",
    "    Transition(\"rock1\", \"jump0\", \"rock0\", 0.9, 0),\n",
    "    Transition(\"rock1\", \"jump1\", \"rock1\", 0.1, 0),\n",
    "    Transition(\"rock1\", \"jump1\", \"land\", 0.9, 10),\n",
    "    Transition(\"rock1\", \"stay\", \"rock1\", 1, 0),\n",
    "    Transition(\"land\", \"stay\", \"land\", 1, 0),\n",
    "    Transition(\"land\", \"jump0\", \"land\", 1, 0),\n",
    "    Transition(\"land\", \"jump1\", \"land\", 1, 0),\n",
    ")\n",
    "# solver = ValueIteration(mdp, CONSTANTS[\"gamma\"])\n",
    "# policy, valuefunc = solver.solve()\n",
    "# print(\"---Value Iteration---\")\n",
    "# print(\"Policy:\")\n",
    "# print(solver.decode_policy(policy))\n",
    "# print(\"Value Function\")\n",
    "# print(valuefunc)\n",
    "\n",
    "solver = PolicyIteration(mdp, CONSTANTS[\"gamma\"])\n",
    "policy, valuefunc = solver.solve()\n",
    "print(\"---Policy Iteration---\")\n",
    "print(\"Policy:\")\n",
    "print(solver.decode_policy(policy))\n",
    "print(\"Value Function\")\n",
    "print(valuefunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a9c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Optional, Hashable\n",
    "\n",
    "from rl2023.constants import EX1_CONSTANTS as CONSTANTS\n",
    "from rl2023.exercise1.mdp import MDP, Transition, State, Action\n",
    "\n",
    "class MDPSolver(ABC):\n",
    "    \"\"\"Base class for MDP solvers\n",
    "\n",
    "    **DO NOT CHANGE THIS CLASS**\n",
    "\n",
    "    :attr mdp (MDP): MDP to solve\n",
    "    :attr gamma (float): discount factor gamma to use\n",
    "    :attr action_dim (int): number of actions in the MDP\n",
    "    :attr state_dim (int): number of states in the MDP\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mdp: MDP, gamma: float):\n",
    "        \"\"\"Constructor of MDPSolver\n",
    "\n",
    "        Initialises some variables from the MDP, namely the state and action dimension variables\n",
    "\n",
    "        :param mdp (MDP): MDP to solve\n",
    "        :param gamma (float): discount factor (gamma)\n",
    "        \"\"\"\n",
    "        self.mdp: MDP = mdp\n",
    "        self.gamma: float = gamma\n",
    "\n",
    "        self.action_dim: int = len(self.mdp.actions)\n",
    "        self.state_dim: int = len(self.mdp.states)\n",
    "\n",
    "    def decode_policy(self, policy: Dict[int, np.ndarray]) -> Dict[State, Action]:\n",
    "        \"\"\"Generates greedy, deterministic policy dict\n",
    "\n",
    "        Given a stochastic policy from state indeces to distribution over actions, the greedy,\n",
    "        deterministic policy is generated choosing the action with highest probability\n",
    "\n",
    "        :param policy (Dict[int, np.ndarray of float with dim (num of actions)]):\n",
    "            stochastic policy assigning a distribution over actions to each state index\n",
    "        :return (Dict[State, Action]): greedy, deterministic policy from states to actions\n",
    "        \"\"\"\n",
    "        new_p = {}\n",
    "        for state, state_idx in self.mdp._state_dict.items():\n",
    "            new_p[state] = self.mdp.actions[np.argmax(policy[state_idx])]\n",
    "        return new_p\n",
    "\n",
    "    @abstractmethod\n",
    "    def solve(self):\n",
    "        \"\"\"Solves the given MDP\n",
    "        \"\"\"\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c49d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
