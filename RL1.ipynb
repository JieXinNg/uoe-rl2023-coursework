{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9bcfeb",
   "metadata": {},
   "source": [
    "Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7dd223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/s1859154/Desktop/Uni%20courses/Reinforcement%20Learning/uoe-rl2023-coursework\n",
      "Requirement already satisfied: numpy>=1.18 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from rl2023==0.1) (1.21.5)\n",
      "Collecting torch>=1.3\n",
      "  Downloading torch-1.13.1-cp39-cp39-win_amd64.whl (162.5 MB)\n",
      "Collecting gym<0.26,>=0.12\n",
      "  Downloading gym-0.25.2.tar.gz (734 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting gym[box2d]\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.41 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from rl2023==0.1) (4.64.0)\n",
      "Collecting pyglet>=1.3\n",
      "  Downloading pyglet-2.0.5-py3-none-any.whl (831 kB)\n",
      "Requirement already satisfied: matplotlib>=3.1 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from rl2023==0.1) (3.5.1)\n",
      "Requirement already satisfied: pytest>=5.3 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from rl2023==0.1) (7.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from gym<0.26,>=0.12->rl2023==0.1) (2.0.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'C:\\Users\\s1859154\\Anaconda3\\python.exe' -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\s1859154\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_zwsos3a\\\\box2d-py_c91050063c0a473f8cc7c67d961fada3\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\s1859154\\\\AppData\\\\Local\\\\Temp\\\\pip-install-_zwsos3a\\\\box2d-py_c91050063c0a473f8cc7c67d961fada3\\\\setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\s1859154\\AppData\\Local\\Temp\\pip-wheel-hz2_vu40'\n",
      "       cwd: C:\\Users\\s1859154\\AppData\\Local\\Temp\\pip-install-_zwsos3a\\box2d-py_c91050063c0a473f8cc7c67d961fada3\\\n",
      "  Complete output (16 lines):\n",
      "  Using setuptools (version 61.2.0).\n",
      "  running bdist_wheel"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from gym<0.26,>=0.12->rl2023==0.1) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gym<0.26,>=0.12->rl2023==0.1) (3.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from matplotlib>=3.1->rl2023==0.1) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from matplotlib>=3.1->rl2023==0.1) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from matplotlib>=3.1->rl2023==0.1) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from matplotlib>=3.1->rl2023==0.1) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from matplotlib>=3.1->rl2023==0.1) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from matplotlib>=3.1->rl2023==0.1) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from matplotlib>=3.1->rl2023==0.1) (0.11.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from pytest>=5.3->rl2023==0.1) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from pytest>=5.3->rl2023==0.1) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from pytest>=5.3->rl2023==0.1) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from pytest>=5.3->rl2023==0.1) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from pytest>=5.3->rl2023==0.1) (1.2.2)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from pytest>=5.3->rl2023==0.1) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from pytest>=5.3->rl2023==0.1) (0.4.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.1->rl2023==0.1) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\s1859154\\anaconda3\\lib\\site-packages (from torch>=1.3->rl2023==0.1) (4.1.1)\n",
      "Collecting gym[box2d]\n",
      "  Downloading gym-0.26.1.tar.gz (719 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "  Downloading gym-0.26.0.tar.gz (710 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting pygame==2.1.0\n",
      "  Downloading pygame-2.1.0-cp39-cp39-win_amd64.whl (4.8 MB)\n",
      "Collecting swig==4.*\n",
      "  Downloading swig-4.1.1-py2.py3-none-win_amd64.whl (2.5 MB)\n",
      "Collecting box2d-py==2.3.5\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
      "Building wheels for collected packages: gym, box2d-py\n",
      "  Building wheel for gym (PEP 517): started\n",
      "  Building wheel for gym (PEP 517): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.25.2-py3-none-any.whl size=852315 sha256=2536a06196a949ac9e631df5a0fcb7b0f41f6d7d4037f1f42689049fe1ca5155\n",
      "  Stored in directory: c:\\users\\s1859154\\appdata\\local\\pip\\cache\\wheels\\05\\4d\\6c\\d0ef0db36695ce032fe20099e3149d8db85cf36656176ff745\n",
      "  Building wheel for box2d-py (setup.py): started\n",
      "  Building wheel for box2d-py (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for box2d-py\n",
      "Successfully built gym\n",
      "Failed to build box2d-py\n",
      "Installing collected packages: gym-notices, swig, pygame, gym, box2d-py, torch, pyglet, rl2023\n",
      "    Running setup.py install for box2d-py: started\n",
      "    Running setup.py install for box2d-py: finished with status 'done'\n",
      "  Running setup.py develop for rl2023\n",
      "Successfully installed box2d-py-2.3.5 gym-0.25.2 gym-notices-0.0.8 pygame-2.1.0 pyglet-2.0.5 rl2023-0.1 swig-4.1.1 torch-1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-3.9\n",
      "  creating build\\lib.win-amd64-3.9\\Box2D\n",
      "  copying library\\Box2D\\Box2D.py -> build\\lib.win-amd64-3.9\\Box2D\n",
      "  copying library\\Box2D\\__init__.py -> build\\lib.win-amd64-3.9\\Box2D\n",
      "  creating build\\lib.win-amd64-3.9\\Box2D\\b2\n",
      "  copying library\\Box2D\\b2\\__init__.py -> build\\lib.win-amd64-3.9\\Box2D\\b2\n",
      "  running build_ext\n",
      "  building 'Box2D._Box2D' extension\n",
      "  swigging Box2D\\Box2D.i to Box2D\\Box2D_wrap.cpp\n",
      "  swig.exe -python -c++ -IBox2D -small -O -includeall -ignoremissing -w201 -globals b2Globals -outdir library\\Box2D -keyword -w511 -D_SWIG_KWARGS -o Box2D\\Box2D_wrap.cpp Box2D\\Box2D.i\n",
      "  error: command 'swig.exe' failed: None\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for box2d-py\n",
      "  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. You can find discussion regarding this at https://github.com/pypa/pip/issues/8368.\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69140367",
   "metadata": {},
   "source": [
    "# Q1 Dynamic Programming\n",
    "- To find optimal policies for Markov Decision Processes (MDPs)\n",
    "- Implement the Policy Iteration (PI) and Value Iteration (VI) algorithms (functions).\n",
    "- For each algorithm, you can find the functions that you need to implement under Tasks below. \n",
    "- Read the code documentation to understand the input and required outputs of these functions. \n",
    "- We will mark your submission only based on the correctness of the outputs of these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd33929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rl2023.exercise1.mdp_solver import ValueIteration, PolicyIteration\n",
    "from rl2023.exercise1.mdp import MDP, Transition, State, Action\n",
    "from rl2023 import constants\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e3a0ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.8}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constants.EX1_CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60facd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_state_values(policy, rewards, gamma=0.9, theta=0.01):\n",
    "    \"\"\"\n",
    "  Args:\n",
    "      policy (np.array): Policy giving the probability of taking each action from each state.\n",
    "      rewards (np.array): Rewards corresponding to reaching the next state from each state (so r(s,s') here rather than the usual r(s,a)).\n",
    "      gamma (float, optional): Discount factor. Defaults to 0.9.\n",
    "      theta (float, optional): Minimum value change threshold to terminate iteration. Defaults to 0.01.\n",
    "\n",
    "  Returns:\n",
    "      values (np.array): State values for given policy and reward mappings.\n",
    "  \"\"\"    \n",
    "    num_states, num_actions = policy.shape # 2, 3\n",
    "    values = np.zeros((num_states+1)) # Include terminal state\n",
    "    \n",
    "    print(f'Beginning policy evaluation for given policy and MDP...\\n')\n",
    "    iteration = 1\n",
    "\n",
    "    while True:\n",
    "        print(f'Iteration: {iteration} \\t Current Values: {values}')\n",
    "        delta=0\n",
    "        initial_values = values\n",
    "        values = np.zeros_like(values)\n",
    "        for state in range(num_states):\n",
    "            for action in range(num_actions):\n",
    "                # With probability 0.9, next state = action with corresponding reward. With probability 0.1, next state = state with no reward.\n",
    "                next_state_probabilities = {action:0.9, state:0.1}\n",
    "                # Often we leave this next state computation (transition dynamics) to the environment. But to use complete dynamic programming we must know \n",
    "                # the transition probablities.\n",
    "                for next_state in next_state_probabilities.keys():\n",
    "                  # Note the expectation here is over both the policy and next state probabilities (environment dynamics)\n",
    "                    aa = policy[state][action]*next_state_probabilities[next_state] * (rewards[state][next_state] + gamma * initial_values[next_state])\n",
    "                    values[state] += aa\n",
    "#                     print(aa)\n",
    "\n",
    "            delta = max(delta, abs(initial_values[state]-values[state]))\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f'\\nMax difference in state value from previous iteration = {delta} which is less than threshold {theta}. Policy Evaluation terminating...\\n')\n",
    "            break\n",
    "\n",
    "        iteration+=1\n",
    "\n",
    "    print(f'Final policy state values: {values}')\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89835e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_greedy_policy(policy, rewards, values, valid_actions, gamma=0.9):\n",
    "    \"\"\"Improve policy (take greedy actions) with respect to rewards and state values.\n",
    "\n",
    "  Args:\n",
    "      policy (np.array): Policy giving the probability of taking each action from each state.\n",
    "      rewards (np.array): Rewards corresponding to reaching the next state from each state (so r(s,s') here rather than the usual r(s,a)).\n",
    "      values (np.array): State values evaluated for current policy.\n",
    "      gamma (float, optional): Discount factor. Defaults to 0.9.\n",
    "\n",
    "  Returns:\n",
    "      policy (np.array): Improved (greedy) policy.\n",
    "  \"\"\"\n",
    "\n",
    "    num_states, num_actions = policy.shape # 2, 3\n",
    "    greedy_policy = np.zeros_like(policy)\n",
    "    state_action_values = np.nan*policy\n",
    "    for action in range(num_actions):\n",
    "        for state in range(num_states):\n",
    "            if valid_actions[state, action]==1:\n",
    "                state_action_values[state][action] = 0.0\n",
    "                # With probability 0.9, next state = action with corresponding reward. With probability 0.1, next state = state with no reward.\n",
    "                next_state_probabilities = {action:0.9, state:0.1}\n",
    "                # Often the environment handles this next state computation (transition dynamics). But to use complete dynamic programming we must know \n",
    "                # the transition probablities.\n",
    "        for next_state in next_state_probabilities.keys():\n",
    "          # Note the expectation is now only over next state probabilities (environment dynamics) since we want the value for each valid state and action.\n",
    "            state_action_values[state][action] += next_state_probabilities[next_state]*(rewards[state][next_state]+gamma*values[next_state])\n",
    "            \n",
    "    \n",
    "    greedy_action = np.nanargmax(state_action_values[state]) # Argmax ignoring invalid actions with nan value\n",
    "  \n",
    "    for action in range(num_actions):\n",
    "        greedy_policy[state][action] = 1 if action == greedy_action else 0\n",
    "\n",
    "    print(f'State action values with previous policy:\\n {state_action_values}\\n')\n",
    "    print(f'Greedy policy after policy improvement:\\n {greedy_policy}')\n",
    "    return greedy_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b1f399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning policy evaluation for given policy and MDP...\n",
      "\n",
      "Iteration: 1 \t Current Values: [0. 0. 0.]\n",
      "Iteration: 2 \t Current Values: [0.  4.5 0. ]\n",
      "Iteration: 3 \t Current Values: [3.645 4.905 0.   ]\n",
      "Iteration: 4 \t Current Values: [4.3011   6.417675 0.      ]\n",
      "Iteration: 5 \t Current Values: [5.58541575 6.81953625 0.        ]\n",
      "Iteration: 6 \t Current Values: [6.02651178 7.37585164 0.        ]\n",
      "Iteration: 7 \t Current Values: [6.51682589 7.60456392 0.        ]\n",
      "Iteration: 8 \t Current Values: [6.7462111  7.82372524 0.        ]\n",
      "Iteration: 9 \t Current Values: [6.94437644 7.93635077 0.        ]\n",
      "Iteration: 10 \t Current Values: [7.053438   8.02674403 0.        ]\n",
      "Iteration: 11 \t Current Values: [7.13647208 8.07904935 0.        ]\n",
      "Iteration: 12 \t Current Values: [7.18631246 8.11738564 0.        ]\n",
      "Iteration: 13 \t Current Values: [7.22185049 8.14102126 0.        ]\n",
      "Iteration: 14 \t Current Values: [7.24419376 8.15754136 0.        ]\n",
      "Iteration: 15 \t Current Values: [7.25958594 8.1680772  0.        ]\n",
      "\n",
      "Max difference in state value from previous iteration = 0.009919322818822351 which is less than threshold 0.01. Policy Evaluation terminating...\n",
      "\n",
      "Final policy state values: [7.26950526 8.17525925 0.        ]\n",
      "State action values with previous policy:\n",
      " [[       nan 0.                nan]\n",
      " [6.6240726         nan 9.73577333]]\n",
      "\n",
      "Greedy policy after policy improvement:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# state values\n",
    "valid_actions = np.array([[0,1,0],[1,0,1]])\n",
    "\n",
    "policy = np.array([[0,1,0],[0.5,0,0.5]])\n",
    "\n",
    "rewards = np.array([[0,0,0],[0,0,10]])\n",
    "###############################################################\n",
    "\n",
    "# algorithm\n",
    "values = calculate_state_values(policy, rewards, gamma=0.9, theta=0.01)\n",
    "\n",
    "policy = calculate_greedy_policy(policy, rewards, values, valid_actions, gamma=0.9)\n",
    "\n",
    "# values = calculate_state_values(policy, rewards, gamma=0.9, theta=0.01)\n",
    "\n",
    "# policy = calculate_greedy_policy(policy, rewards, values, valid_actions, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "300a0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration(MDPSolver):\n",
    "    \"\"\"MDP solver using the Policy Iteration algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def _policy_eval(self): #, policy: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Computes one policy evaluation step\n",
    "\n",
    "        **YOU MUST IMPLEMENT THIS FUNCTION FOR Q1**\n",
    "\n",
    "        :param policy (np.ndarray of float with dim (num of states, num of actions)):\n",
    "            A 2D NumPy array that encodes the policy.\n",
    "            It is indexed as (STATE, ACTION) where policy[STATE, ACTION] has the probability of\n",
    "            taking action 'ACTION' in state 'STATE'.\n",
    "            REMEMBER: the sum of policy[STATE, :] should always be 1.0\n",
    "            For deterministic policies the following holds for each state S:\n",
    "            policy[S, BEST_ACTION] = 1.0\n",
    "            policy[S, OTHER_ACTIONS] = 0\n",
    "        :return (np.ndarray of float with dim (num of states)): \n",
    "            A 1D NumPy array that encodes the computed value function\n",
    "            It is indexed as (State) where V[State] is the value of state 'State'\n",
    "        \"\"\"\n",
    "        ### PUT YOUR CODE HERE ###\n",
    "        # initilise V(s)\n",
    "        theta = 0.01 # self.theta\n",
    "#         policy = np.zeros([self.state_dim, self.action_dim])\n",
    "#         policy = np.array([[0,0.5,0.5],[1,0,0],[1,0,0]])\n",
    "#         policy = np.array([[0,0,1],[1,0,0],[0.5,0.5,0]])\n",
    "        policy = np.array([[1,0,0],[0,1,0],[0.5,0,0.5]])\n",
    "        num_states, num_actions = policy.shape # 2, 3\n",
    "    \n",
    "        V = np.zeros(self.state_dim)\n",
    "#         values = np.zeros((num_states + 1)) # Include terminal state\n",
    "        gamma = 0.9 #self.gamma # obtain gamma value\n",
    "        \n",
    "        rewards = self.mdp.R # access rewards \n",
    "        print(rewards)\n",
    "        print(self.mdp.P) \n",
    "\n",
    "        \n",
    "        print(f'Beginning policy evaluation for given policy and MDP...\\n')\n",
    "        iteration = 1\n",
    "\n",
    "        while True:\n",
    "            print(f'Iteration: {iteration} \\t Current Values: {V}')\n",
    "            delta = 0\n",
    "            # set v = V\n",
    "            initial_values = V\n",
    "            # V(s) = values\n",
    "            V = np.zeros_like(V)\n",
    "            for s in range(num_states):\n",
    "                for a in range(num_actions):\n",
    "                    for ns in range(num_states):\n",
    "                        next_state_probabilities = self.mdp.P[s, a, ns] # *** how to get te next state policy?\n",
    "                      # Note the expectation here is over both the policy and next state probabilities (environment dynamics)\n",
    "                        V[s] += policy[s][a]*next_state_probabilities*(rewards[s, a, ns] + gamma * initial_values[ns])\n",
    "#                         print(policy[s][a], next_state_probabilities, rewards[s, a, ns] , initial_values[ns])\n",
    "\n",
    "                delta = max(delta, abs(initial_values[s]-V[s]))\n",
    "        \n",
    "            if delta < theta:\n",
    "                print(f'\\nMax difference in state value from previous iteration = {delta} which is less than threshold {theta}. Policy Evaluation terminating...\\n')\n",
    "                break\n",
    "\n",
    "            iteration+=1\n",
    "\n",
    "            print(f'Final policy state values: {V}')\n",
    "        \n",
    "        \n",
    "#         raise NotImplementedError(\"Needed for Q1\")\n",
    "        return np.array(V)\n",
    "\n",
    "    def _policy_improvement(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Computes policy iteration until a stable policy is reached\n",
    "\n",
    "        **YOU MUST IMPLEMENT THIS FUNCTION FOR Q1**\n",
    "\n",
    "        Useful Variables (As with Value Iteration):\n",
    "        1. `self.mpd` -- Gives access to the MDP.\n",
    "        2. `self.mdp.R` -- 3D NumPy array with the rewards for each transition.\n",
    "            E.g. the reward of transition [3] -2-> [4] (going from state 3 to state 4 with action\n",
    "            2) can be accessed with `self.R[3, 2, 4]`\n",
    "        3. `self.mdp.P` -- 3D NumPy array with transition probabilities.\n",
    "            *REMEMBER*: the sum of (STATE, ACTION, :) should be 1.0 (all actions lead somewhere)\n",
    "            E.g. the transition probability of transition [3] -2-> [4] (going from state 3 to\n",
    "            state 4 with action 2) can be accessed with `self.P[3, 2, 4]`\n",
    "\n",
    "        :return (Tuple[np.ndarray of float with dim (num of states, num of actions),\n",
    "                       np.ndarray of float with dim (num of states)):\n",
    "            Tuple of calculated policy and value function\n",
    "        \"\"\"\n",
    "        policy = np.zeros([self.state_dim, self.action_dim])\n",
    "        V = np.zeros([self.state_dim])\n",
    "        ### PUT YOUR CODE HERE ###\n",
    "        raise NotImplementedError(\"Needed for Q1\")\n",
    "        return policy, V\n",
    "\n",
    "    def solve(self, theta: float = 1e-6) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Solves the MDP\n",
    "\n",
    "        This function compiles the MDP and then calls the\n",
    "        policy improvement function that the student must implement\n",
    "        and returns the solution\n",
    "\n",
    "        **DO NOT CHANGE THIS FUNCTION**\n",
    "\n",
    "        :param theta (float, optional): stop threshold, \n",
    "        defaults to 1e-6\n",
    "        :return (Tuple[np.ndarray of float with dim (num of states, num of actions),\n",
    "                       np.ndarray of float with dim (num of states)]):\n",
    "            Tuple of calculated policy and value function\n",
    "        \"\"\"\n",
    "        self.mdp.ensure_compiled()\n",
    "        self.theta = theta\n",
    "        return self._policy_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "64b493d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state =  0 action =  0 policy = 0.0\n",
      "state =  0 action =  1 policy = 1.0\n",
      "state =  0 action =  2 policy = 0.0\n",
      "state =  1 action =  0 policy = 0.5\n",
      "state =  1 action =  1 policy = 0.0\n",
      "state =  1 action =  2 policy = 0.5\n",
      "state =  2 action =  0 policy = 0.0\n",
      "state =  2 action =  1 policy = 0.0\n",
      "state =  2 action =  2 policy = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0. , 1. , 0. ],\n",
       "       [0.5, 0. , 0.5],\n",
       "       [0. , 0. , 0. ]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # rock 0, rock 1, land\n",
    "policy = np.array([[0,1,0],[0.5,0,0.5],[0,0,0]])\n",
    "# # policy = policy.transpose()\n",
    "# # rock 1, land, rock 0\n",
    "# policy = np.array([[0,0.5,0.5],[0,0,0],[1,0,0]])\n",
    "\n",
    "for s in range(3):\n",
    "    for a in range(3):\n",
    "        print(\"state = \", s, \"action = \", a, \"policy =\", policy[s][a] )\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d58670fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]]\n",
      "\n",
      " [[ 0. 10.  0.]\n",
      "  [ 0.  0.  0.]\n",
      "  [ 0.  0.  0.]]]\n",
      "[[[0.1 0.  0.9]\n",
      "  [1.  0.  0. ]\n",
      "  [1.  0.  0. ]]\n",
      "\n",
      " [[0.  1.  0. ]\n",
      "  [0.  1.  0. ]\n",
      "  [0.  1.  0. ]]\n",
      "\n",
      " [[0.  0.9 0.1]\n",
      "  [0.  0.  1. ]\n",
      "  [0.9 0.  0.1]]]\n",
      "Beginning policy evaluation for given policy and MDP...\n",
      "\n",
      "Iteration: 1 \t Current Values: [0. 0. 0.]\n",
      "Final policy state values: [0.  0.  4.5]\n",
      "Iteration: 2 \t Current Values: [0.  0.  4.5]\n",
      "Final policy state values: [3.645 0.    4.905]\n",
      "Iteration: 3 \t Current Values: [3.645 0.    4.905]\n",
      "Final policy state values: [4.3011   0.       6.417675]\n",
      "Iteration: 4 \t Current Values: [4.3011   0.       6.417675]\n",
      "Final policy state values: [5.58541575 0.         6.81953625]\n",
      "Iteration: 5 \t Current Values: [5.58541575 0.         6.81953625]\n",
      "Final policy state values: [6.02651178 0.         7.37585164]\n",
      "Iteration: 6 \t Current Values: [6.02651178 0.         7.37585164]\n",
      "Final policy state values: [6.51682589 0.         7.60456392]\n",
      "Iteration: 7 \t Current Values: [6.51682589 0.         7.60456392]\n",
      "Final policy state values: [6.7462111  0.         7.82372524]\n",
      "Iteration: 8 \t Current Values: [6.7462111  0.         7.82372524]\n",
      "Final policy state values: [6.94437644 0.         7.93635077]\n",
      "Iteration: 9 \t Current Values: [6.94437644 0.         7.93635077]\n",
      "Final policy state values: [7.053438   0.         8.02674403]\n",
      "Iteration: 10 \t Current Values: [7.053438   0.         8.02674403]\n",
      "Final policy state values: [7.13647208 0.         8.07904935]\n",
      "Iteration: 11 \t Current Values: [7.13647208 0.         8.07904935]\n",
      "Final policy state values: [7.18631246 0.         8.11738564]\n",
      "Iteration: 12 \t Current Values: [7.18631246 0.         8.11738564]\n",
      "Final policy state values: [7.22185049 0.         8.14102126]\n",
      "Iteration: 13 \t Current Values: [7.22185049 0.         8.14102126]\n",
      "Final policy state values: [7.24419376 0.         8.15754136]\n",
      "Iteration: 14 \t Current Values: [7.24419376 0.         8.15754136]\n",
      "Final policy state values: [7.25958594 0.         8.1680772 ]\n",
      "Iteration: 15 \t Current Values: [7.25958594 0.         8.1680772 ]\n",
      "\n",
      "Max difference in state value from previous iteration = 0.009919322818822351 which is less than threshold 0.01. Policy Evaluation terminating...\n",
      "\n",
      "---Policy Iteration---\n",
      "Policy:\n",
      "{'rock0': 'jump1', 'land': 'jump1', 'rock1': 'jump1'}\n",
      "[7.26950526 0.         8.17525925]\n"
     ]
    }
   ],
   "source": [
    "mdp = MDP()\n",
    "mdp.add_transition(\n",
    "    #         start action end prob reward\n",
    "    Transition(\"rock0\", \"jump0\", \"rock0\", 1, 0),\n",
    "    Transition(\"rock0\", \"stay\", \"rock0\", 1, 0),\n",
    "    Transition(\"rock0\", \"jump1\", \"rock0\", 0.1, 0),\n",
    "    Transition(\"rock0\", \"jump1\", \"rock1\", 0.9, 0),\n",
    "    Transition(\"rock1\", \"jump0\", \"rock1\", 0.1, 0),\n",
    "    Transition(\"rock1\", \"jump0\", \"rock0\", 0.9, 0),\n",
    "    Transition(\"rock1\", \"jump1\", \"rock1\", 0.1, 0),\n",
    "    Transition(\"rock1\", \"jump1\", \"land\", 0.9, 10),\n",
    "    Transition(\"rock1\", \"stay\", \"rock1\", 1, 0),\n",
    "    Transition(\"land\", \"stay\", \"land\", 1, 0),\n",
    "    Transition(\"land\", \"jump0\", \"land\", 1, 0),\n",
    "    Transition(\"land\", \"jump1\", \"land\", 1, 0),\n",
    ")\n",
    "# mdp.transitions\n",
    "solver = PolicyIteration(mdp, CONSTANTS[\"gamma\"])\n",
    "policy = solver.solve()\n",
    "print(\"---Policy Iteration---\")\n",
    "print(\"Policy:\")\n",
    "print(solver.decode_policy(policy))\n",
    "# # print(\"Value Function\")\n",
    "# # print(valuefunc)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39a66e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     mdp = MDP()\n",
    "#     mdp.add_transition(\n",
    "#         #         start action end prob reward\n",
    "#         Transition(\"rock0\", \"jump0\", \"rock0\", 1, 0),\n",
    "#         Transition(\"rock0\", \"stay\", \"rock0\", 1, 0),\n",
    "#         Transition(\"rock0\", \"jump1\", \"rock0\", 0.1, 0),\n",
    "#         Transition(\"rock0\", \"jump1\", \"rock1\", 0.9, 0),\n",
    "#         Transition(\"rock1\", \"jump0\", \"rock1\", 0.1, 0),\n",
    "#         Transition(\"rock1\", \"jump0\", \"rock0\", 0.9, 0),\n",
    "#         Transition(\"rock1\", \"jump1\", \"rock1\", 0.1, 0),\n",
    "#         Transition(\"rock1\", \"jump1\", \"land\", 0.9, 10),\n",
    "#         Transition(\"rock1\", \"stay\", \"rock1\", 1, 0),\n",
    "#         Transition(\"land\", \"stay\", \"land\", 1, 0),\n",
    "#         Transition(\"land\", \"jump0\", \"land\", 1, 0),\n",
    "#         Transition(\"land\", \"jump1\", \"land\", 1, 0),\n",
    "#     )\n",
    "\n",
    "#     solver = ValueIteration(mdp, CONSTANTS[\"gamma\"])\n",
    "#     policy, valuefunc = solver.solve()\n",
    "#     print(\"---Value Iteration---\")\n",
    "#     print(\"Policy:\")\n",
    "#     print(solver.decode_policy(policy))\n",
    "#     print(\"Value Function\")\n",
    "#     print(valuefunc)\n",
    "\n",
    "#     solver = PolicyIteration(mdp, CONSTANTS[\"gamma\"])\n",
    "#     policy, valuefunc = solver.solve()\n",
    "#     print(\"---Policy Iteration---\")\n",
    "#     print(\"Policy:\")\n",
    "#     print(solver.decode_policy(policy))\n",
    "#     print(\"Value Function\")\n",
    "#     print(valuefunc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33a9c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Optional, Hashable\n",
    "\n",
    "from rl2023.constants import EX1_CONSTANTS as CONSTANTS\n",
    "from rl2023.exercise1.mdp import MDP, Transition, State, Action\n",
    "\n",
    "class MDPSolver(ABC):\n",
    "    \"\"\"Base class for MDP solvers\n",
    "\n",
    "    **DO NOT CHANGE THIS CLASS**\n",
    "\n",
    "    :attr mdp (MDP): MDP to solve\n",
    "    :attr gamma (float): discount factor gamma to use\n",
    "    :attr action_dim (int): number of actions in the MDP\n",
    "    :attr state_dim (int): number of states in the MDP\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mdp: MDP, gamma: float):\n",
    "        \"\"\"Constructor of MDPSolver\n",
    "\n",
    "        Initialises some variables from the MDP, namely the state and action dimension variables\n",
    "\n",
    "        :param mdp (MDP): MDP to solve\n",
    "        :param gamma (float): discount factor (gamma)\n",
    "        \"\"\"\n",
    "        self.mdp: MDP = mdp\n",
    "        self.gamma: float = gamma\n",
    "\n",
    "        self.action_dim: int = len(self.mdp.actions)\n",
    "        self.state_dim: int = len(self.mdp.states)\n",
    "\n",
    "    def decode_policy(self, policy: Dict[int, np.ndarray]) -> Dict[State, Action]:\n",
    "        \"\"\"Generates greedy, deterministic policy dict\n",
    "\n",
    "        Given a stochastic policy from state indeces to distribution over actions, the greedy,\n",
    "        deterministic policy is generated choosing the action with highest probability\n",
    "\n",
    "        :param policy (Dict[int, np.ndarray of float with dim (num of actions)]):\n",
    "            stochastic policy assigning a distribution over actions to each state index\n",
    "        :return (Dict[State, Action]): greedy, deterministic policy from states to actions\n",
    "        \"\"\"\n",
    "        new_p = {}\n",
    "        for state, state_idx in self.mdp._state_dict.items():\n",
    "            new_p[state] = self.mdp.actions[np.argmax(policy[state_idx])]\n",
    "        return new_p\n",
    "\n",
    "    @abstractmethod\n",
    "    def solve(self):\n",
    "        \"\"\"Solves the given MDP\n",
    "        \"\"\"\n",
    "        ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
